# Echo

**A modular AI infrastructure platform for creative technologists.**

Echo breaks free from monolithic AI platforms by treating intelligence as composable modules. Mix and match LLM providers, integrate MCP-compliant tools, and orchestrate sophisticated AI workflowsâ€”all through a unified, vendor-agnostic interface.

Built Docker-first for maximum portability. No vendor lock-in. Maximum flexibility.

---

## ðŸŽ¯ Vision: Modular AI Composition

Traditional AI platforms lock you into their ecosystem. Echo does the oppositeâ€”it's designed as an orchestration layer that composes heterogeneous AI capabilities:

- **ðŸ§  LLM Providers**: OpenAI, Anthropic Claude, local Ollama models, or emerging providers
- **ðŸ”§ MCP Tools**: Web search, file operations, databases, APIsâ€”any MCP-compliant server
- **ðŸ¤– Agent Workflows**: Chain reasoning, tool usage, and decision logic into autonomous processes
- **ðŸ”„ Dynamic Discovery**: Tools and capabilities are discovered at runtime, not hardcoded

All orchestrated through a single, clean interface that adapts to your workflow.

---

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚    Backend      â”‚    â”‚  External APIs  â”‚
â”‚   (Nginx)       â”‚â”€â”€â”€â”€â”‚   (FastAPI)     â”‚â”€â”€â”€â”€â”‚  & MCP Servers  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Modern UI     â”‚    â”‚ â€¢ LLM Router    â”‚    â”‚ â€¢ OpenAI/Claude â”‚
â”‚ â€¢ Tool Display  â”‚    â”‚ â€¢ MCP Client    â”‚    â”‚ â€¢ Ollama        â”‚
â”‚ â€¢ Live Updates  â”‚    â”‚ â€¢ Tool Executor â”‚    â”‚ â€¢ Custom Tools  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- **Frontend**: Modern, dark-mode chat interface served by Nginx with live tool discovery
- **Backend**: FastAPI server with pluggable LLM routing and async MCP integration
- **LLM Router**: Intelligent model selection based on task requirements (OpenAI, Claude, Ollama)
- **MCP Client**: Dynamic tool discovery, schema validation, and execution
- **Tool Orchestration**: Automatic tool detection and intelligent parameter mapping

---

## âœ¨ Features

### Current Capabilities
- ðŸŽ¨ **Modern Chat Interface**: Claude-style dark UI with real-time tool visibility
- ðŸ”Œ **Modular LLM Support**: Easy switching between OpenAI, Claude, and local models
- ðŸ› ï¸ **MCP Tool Integration**: Automatic discovery and execution of MCP-compliant tools
- ðŸ³ **Docker-First**: Fully containerized for development and production
- ðŸ”„ **Live Reload**: Code changes reflected instantly in development
- ðŸ“Š **Tool Transparency**: See which tools are used and their results
- âš¡ **Async Everything**: Non-blocking tool discovery and execution

### Smart Tool Detection
Echo automatically detects when to use tools based on message content:
- **Math operations** â†’ Calculator tools
- **Web queries** â†’ Search tools
- **File operations** â†’ File system tools
- **API calls** â†’ Custom MCP servers

---

## ðŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- OpenAI API key (optional: Claude API key, Ollama setup)

### 1. Clone and Configure
```bash
git clone https://github.com/coltonbatts/Echo.git
cd Echo
cp .env.example .env
# Edit .env with your API keys
```

### 2. Launch with Docker (Recommended)
```bash
docker compose up --build
```

- **ðŸŒ Frontend**: [http://localhost](http://localhost)
- **ðŸ“š API Docs**: [http://localhost:8000/docs](http://localhost:8000/docs)
- **ðŸ”§ Tools API**: [http://localhost:8000/api/tools](http://localhost:8000/api/tools)

### 3. Local Development (Advanced)
```bash
# Terminal 1: Backend
pip install -r backend/requirements.txt
uvicorn backend.main:app --reload

# Terminal 2: Open frontend/index.html or serve it
python -m http.server 3000 --directory frontend
```

---
git clone https://github.com/coltonbatts/Echo.git
cd Echo
```

### 2. Set up configuration
- Copy `.env.example` to `.env` and add your API keys or endpoints as needed.
- **Never commit real keys to version control!**

### 3. Local Development
1. **Install dependencies**
   ```bash
   pip install fastapi uvicorn openai pyyaml
   ```
2. **Start the backend**
   ```bash
   uvicorn backend.main:app --reload
   ```
3. **Open the frontend**
   - Open `http://localhost:8000` in your browser.

### 4. Docker (Recommended for full isolation)
1. **Build and run everything**
    ```bash
    docker compose up --build
    ```
    - **Frontend:** [http://localhost](http://localhost) â† served by Nginx container, proxies API calls to backend
    - **Backend:** [http://localhost:8000](http://localhost:8000) (API only; not for direct user access)
    - **API docs:** [http://localhost:8000/docs](http://localhost:8000/docs)
2. **Live reload:** Code changes are reflected automatically in the containers if you use volume mounts (default in `docker-compose.yml`).
    - To force reload, restart the containers: `docker compose restart`
   - API docs: [http://localhost:8000/docs](http://localhost:8000/docs)
2. **Frontend:** Open `frontend/index.html` (not served by backend)
3. **Live reload:** Code changes are reflected automatically in the container.

---

## API Endpoints
- `POST /api/echo` â€“ Main chat endpoint (expects JSON, returns LLM response)
- `GET /docs` â€“ FastAPI interactive API docs
- `GET /` â€“ Not implemented (returns 404 by design)

---

## Development Notes
- **Frontend:** Pure HTML/CSS, no build step, open directly.
- **Backend:** Modular FastAPI app, ready for LLM and MCP integration.
- **Docker:** Lightweight, production-ready config. Mounts code for live reload.
- **Extending:** Add new LLMs/tools by editing `llm_router.py` and `mcp_client.py`.

### Running Tests
Echo includes a small pytest suite under `tests/`.

```bash
pip install -r backend/requirements.txt
pip install pytest respx
pytest
```

---

## Roadmap
- Real LLM routing (OpenAI, Claude, Ollama, etc)
- Live MCP tool discovery and invocation
- Desktop app via Tauri
- Persistent chat history

---

> Echo is a personal creative-tech companion. Not for corporate use.
